{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784e2ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Merge Process ---\n",
      "Loaded Microbiome Data. Shape: (169, 36)\n",
      "Loaded Metadata. Shape: (170, 18)\n",
      "Metadata columns: ['ID', 'LinkerPrimerSequence', 'ReversePrimer', 'barcode', 'Plate', 'WellPosition', 'mice_name', 'date', 'date_month', 'number', 'age (weeks)', 'ignore_kit', 'group', 'death', 'metabolomics', 'death_date', 'death_age_week', 'death_age_month']\n",
      "--- Merge Complete ---\n",
      "Original Microbiome Samples: 169\n",
      "Original Metadata Samples: 170\n",
      "Merged Samples (Intersection): 169\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Paths ===\n",
    "# Input: The microbiome data we just created (Level 7)\n",
    "microbiome_path = \"/home/pintokf/Projects/Microbium/Mouses/MIPMLP_scripts/whole_metadata/processed_subpca_level7.csv\"\n",
    "\n",
    "# Input: The metadata file\n",
    "metadata_path = \"/home/pintokf/Projects/Microbium/Mouses/mouses_2_data/metadata_ok173_time_series_all.txt\"\n",
    "\n",
    "# Output: The merged file ready for the next preprocessing steps\n",
    "#output_path = \"/home/pintokf/Projects/Microbium/Mouses/Ratio_model/merged_data_level6.csv\"\n",
    "\n",
    "print(\"--- Starting Merge Process ---\")\n",
    "\n",
    "# 1. Load Microbiome Data\n",
    "try:\n",
    "    df_micro = pd.read_csv(microbiome_path)\n",
    "    print(f\"Loaded Microbiome Data. Shape: {df_micro.shape}\")\n",
    "    # Verify ID exists\n",
    "    if 'ID' not in df_micro.columns:\n",
    "        raise ValueError(\"Column 'ID' not found in microbiome data!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading microbiome data: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 2. Load Metadata\n",
    "try:\n",
    "    # Assuming metadata is tab-separated based on previous format\n",
    "    df_meta = pd.read_csv(metadata_path, sep='\\t')\n",
    "    print(f\"Loaded Metadata. Shape: {df_meta.shape}\")\n",
    "    \n",
    "    # Standardize ID column name in Metadata\n",
    "    # Common QIIME formats use '#SampleID' or 'SampleID'\n",
    "    if '#SampleID' in df_meta.columns:\n",
    "        df_meta.rename(columns={'#SampleID': 'ID'}, inplace=True)\n",
    "    elif 'SampleID' in df_meta.columns:\n",
    "        df_meta.rename(columns={'SampleID': 'ID'}, inplace=True)\n",
    "        \n",
    "    print(f\"Metadata columns: {list(df_meta.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading metadata: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 3. Merge Tables\n",
    "# We use 'inner' merge to keep only samples that have BOTH microbiome and metadata info\n",
    "merged_df = pd.merge(df_micro, df_meta, on='ID', how='inner')\n",
    "\n",
    "print(f\"--- Merge Complete ---\")\n",
    "print(f\"Original Microbiome Samples: {len(df_micro)}\")\n",
    "print(f\"Original Metadata Samples: {len(df_meta)}\")\n",
    "print(f\"Merged Samples (Intersection): {len(merged_df)}\")\n",
    "\n",
    "# 4. Save\n",
    "#merged_df.to_csv(output_path, index=False)\n",
    "#print(f\"✅ Saved merged file to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f20b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved merged file to: /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/Preprocces_ratio_microbiome/merged_data_level7.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/Preprocces_ratio_microbiome/merged_data_level7.csv\"\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Saved merged file to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e901d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Total: 169\n",
      "Uncensored (Dead/Events): 49\n",
      "Censored (Alive/No Event): 120\n",
      "\n",
      "✅ Files saved successfully:\n",
      "1. /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/Preprocces_ratio_microbiome/data_level7_uncensored.csv\n",
      "2. /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/Preprocces_ratio_microbiome/data_level7_censored.csv\n"
     ]
    }
   ],
   "source": [
    "# === Split Data based on 'Death' column ===\n",
    "\n",
    "# 1. Create the Uncensored group (Death = yes)\n",
    "df_uncensored = merged_df[merged_df['death'] == 'yes'].copy()\n",
    "\n",
    "# 2. Create the Censored group (Death = no)\n",
    "df_censored = merged_df[merged_df['death'] == 'no'].copy()\n",
    "\n",
    "# === Verification ===\n",
    "print(f\"Original Total: {len(merged_df)}\")\n",
    "print(f\"Uncensored (Dead/Events): {len(df_uncensored)}\")\n",
    "print(f\"Censored (Alive/No Event): {len(df_censored)}\")\n",
    "\n",
    "# === Save to files ===\n",
    "output_dir = \"/home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/Preprocces_ratio_microbiome\"\n",
    "\n",
    "path_uncensored = f\"{output_dir}/data_level7_uncensored.csv\"\n",
    "path_censored = f\"{output_dir}/data_level7_censored.csv\"\n",
    "\n",
    "df_uncensored.to_csv(path_uncensored, index=False)\n",
    "df_censored.to_csv(path_censored, index=False)\n",
    "\n",
    "print(f\"\\n✅ Files saved successfully:\")\n",
    "print(f\"1. {path_uncensored}\")\n",
    "print(f\"2. {path_censored}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ccf613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Uncensored (Dead) Data ---\n",
      "✅ Extracted 'Cage' and 'MiceName' from 'mice_name'\n",
      "Preview (should be 2, 4, 6):\n",
      "           ID  AgeMonths\n",
      "2   14-1_5-20          4\n",
      "3   14-1_7-20          6\n",
      "10  16-0_7-20          6\n",
      "21  19-0_5-20          6\n",
      "22  19-0_7-20          6\n",
      "\n",
      "--- Processing Censored (Alive) Data ---\n",
      "✅ Extracted 'Cage' and 'MiceName' from 'mice_name'\n",
      "Preview (should be 2, 4, 6):\n",
      "          ID  AgeMonths\n",
      "0  14-0_5-20          4\n",
      "1  14-0_7-20          6\n",
      "4  14-2_5-20          4\n",
      "5  14-2_7-20          6\n",
      "6  15-0_5-20          4\n",
      "\n",
      "✅ Files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_mouse_data(df_input):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # === 1. Standardize Column Names ===\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Rename 'age (weeks)' to 'AgeMonths'\n",
    "    if 'age (weeks)' in df.columns:\n",
    "        df.rename(columns={'age (weeks)': 'AgeMonths'}, inplace=True)\n",
    "        \n",
    "    # Rename 'date_month' to 'SamplingDate'\n",
    "    if 'date_month' in df.columns:\n",
    "        df.rename(columns={'date_month': 'SamplingDate'}, inplace=True)\n",
    "\n",
    "    # === 2. Extract 'Cage' and 'MiceName' from 'mice_name' ===\n",
    "    if 'mice_name' in df.columns:\n",
    "        # Create 'Cage' column\n",
    "        df['Cage'] = df['mice_name'].apply(lambda x: str(x).split('-')[0] if '-' in str(x) else str(x))\n",
    "        \n",
    "        # Create 'MiceName' column\n",
    "        df['MiceName'] = df['mice_name'].astype(str).str.replace('Agf', '', case=False, regex=False)\n",
    "        df['MiceName'] = df['MiceName'].str.replace('-m', '-', regex=False)\n",
    "        \n",
    "        print(\"✅ Extracted 'Cage' and 'MiceName' from 'mice_name'\")\n",
    "\n",
    "    # === 3. Convert Weeks to Months (Using CEIL / Round UP) ===\n",
    "    # Logic: \n",
    "    # 7 weeks / 4 = 1.75 -> ceil -> 2\n",
    "    # 15 weeks / 4 = 3.75 -> ceil -> 4\n",
    "    # 23 weeks / 4 = 5.75 -> ceil -> 6\n",
    "    if 'AgeMonths' in df.columns:\n",
    "        df['AgeMonths'] = pd.to_numeric(df['AgeMonths'], errors='coerce')\n",
    "        df['AgeMonths'] = np.ceil(df['AgeMonths'] / 4).astype('Int64')\n",
    "\n",
    "    # === 4. Fix SamplingDate Format ===\n",
    "    if 'SamplingDate' in df.columns:\n",
    "        def fix_date_format(val):\n",
    "            val_str = str(val).strip()\n",
    "            if '_' in val_str:\n",
    "                parts = val_str.split('_')\n",
    "                if len(parts) == 2:\n",
    "                    return f\"01-{parts[0]}-20{parts[1]}\"\n",
    "            return val \n",
    "\n",
    "        df['SamplingDate'] = df['SamplingDate'].apply(fix_date_format)\n",
    "        df['SamplingDate'] = pd.to_datetime(df['SamplingDate'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # === 5. Drop unnecessary columns ===\n",
    "    cols_to_drop = ['barcode', 'Plate', 'LinkerPrimerSequence', 'ReversePrimer', \n",
    "                    'death', 'WellPosition', 'metabolomics', 'date', 'number', \n",
    "                    'ignore_kit', 'group', 'death_age_week',\n",
    "                    'mice_name'] \n",
    "    \n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "# === Execution ===\n",
    "\n",
    "print(\"--- Processing Uncensored (Dead) Data ---\")\n",
    "if 'df_uncensored' in locals():\n",
    "    df_uncensored_clean = process_mouse_data(df_uncensored)\n",
    "    print(\"Preview (should be 2, 4, 6):\")\n",
    "    print(df_uncensored_clean[['ID', 'AgeMonths']].head())\n",
    "\n",
    "print(\"\\n--- Processing Censored (Alive) Data ---\")\n",
    "if 'df_censored' in locals():\n",
    "    df_censored_clean = process_mouse_data(df_censored)\n",
    "    print(\"Preview (should be 2, 4, 6):\")\n",
    "    print(df_censored_clean[['ID', 'AgeMonths']].head())\n",
    "\n",
    "# === Save Files ===\n",
    "if 'path_uncensored' in locals() and 'path_censored' in locals():\n",
    "    df_uncensored_clean.to_csv(path_uncensored, index=False)\n",
    "    df_censored_clean.to_csv(path_censored, index=False)\n",
    "    print(f\"\\n✅ Files saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b743d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Uncensored Data (Dates & Diff) ---\n",
      "✅ Added 'diff' column.\n",
      "           ID  AgeMonths  death_age_month  diff\n",
      "2   14-1_5-20          4               17   390\n",
      "3   14-1_7-20          6               17   330\n",
      "10  16-0_7-20          6               15   270\n",
      "21  19-0_5-20          6               13   210\n",
      "22  19-0_7-20          6               13   210\n",
      "✅ Saved updated file to: /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/Preprocces_ratio_microbiome/data_level7_uncensored.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Logic to fix DeathDate specific format ===\n",
    "# Input examples: \"05_21_b\", \"06_21\"\n",
    "# Output: 01-05-2021 (datetime object)\n",
    "def fix_death_date_format(val):\n",
    "    val_str = str(val).strip()\n",
    "    \n",
    "    if val_str == 'nan' or val_str == '':\n",
    "        return pd.NaT\n",
    "    \n",
    "    try:\n",
    "        # Split by underscore '_'\n",
    "        parts = val_str.split('_')\n",
    "        \n",
    "        # We need at least the first two parts (Month and Year)\n",
    "        if len(parts) >= 2:\n",
    "            month = parts[0]      # e.g., \"05\"\n",
    "            year_short = parts[1] # e.g., \"21\"\n",
    "            \n",
    "            # Construct the string \"01-MM-20YY\"\n",
    "            date_str = f\"01-{month}-20{year_short}\"\n",
    "            \n",
    "            # Convert to datetime\n",
    "            return pd.to_datetime(date_str, format='%d-%m-%Y')\n",
    "        else:\n",
    "            return pd.NaT\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error parsing date: {val} -> {e}\")\n",
    "        return pd.NaT\n",
    "\n",
    "# === Main Processing for Uncensored Data ===\n",
    "print(\"--- Processing Uncensored Data (Dates & Diff) ---\")\n",
    "\n",
    "# Ensure the dataframe exists\n",
    "if 'df_uncensored_clean' in locals():\n",
    "    \n",
    "    # 1. Apply DeathDate Fix\n",
    "    if 'death_date' in df_uncensored_clean.columns:\n",
    "        df_uncensored_clean['death_date'] = df_uncensored_clean['death_date'].apply(fix_death_date_format)\n",
    "\n",
    "    # 2. Calculate 'diff' Column\n",
    "    # Formula: (DeathAgeMonths - AgeMonths) * 30\n",
    "    if 'death_age_month' in df_uncensored_clean.columns and 'AgeMonths' in df_uncensored_clean.columns:\n",
    "        \n",
    "        # Ensure columns are numeric\n",
    "        df_uncensored_clean['death_age_month'] = pd.to_numeric(df_uncensored_clean['death_age_month'], errors='coerce')\n",
    "        df_uncensored_clean['AgeMonths'] = pd.to_numeric(df_uncensored_clean['AgeMonths'], errors='coerce')\n",
    "        \n",
    "        # Perform calculation\n",
    "        df_uncensored_clean['diff'] = (df_uncensored_clean['death_age_month'] - df_uncensored_clean['AgeMonths']) * 30\n",
    "        \n",
    "        print(\"✅ Added 'diff' column.\")\n",
    "        # Preview the new column\n",
    "        print(df_uncensored_clean[['ID', 'AgeMonths', 'death_age_month', 'diff']].head())\n",
    "    else:\n",
    "        print(\"❌ Error: Missing 'death_age_month' or 'AgeMonths' columns.\")\n",
    "\n",
    "    # 3. Save to file\n",
    "    df_uncensored_clean = df_uncensored_clean.drop('death_age_month', axis=1)\n",
    "    df_uncensored_clean.to_csv(path_uncensored, index=False)\n",
    "    print(f\"✅ Saved updated file to: {path_uncensored}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Error: df_uncensored_clean is not defined. Please run previous steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07003854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Censored Data (Columns & Dates) ---\n",
      "✅ Added 'diff' and 'DateEnd' columns.\n",
      "          ID  AgeMonths SamplingDate    DateEnd  diff\n",
      "0  14-0_5-20          4   2020-05-01 2021-07-01   420\n",
      "1  14-0_7-20          6   2020-07-01 2021-07-01   360\n",
      "4  14-2_5-20          4   2020-05-01 2021-07-01   420\n",
      "5  14-2_7-20          6   2020-07-01 2021-07-01   360\n",
      "6  15-0_5-20          4   2020-05-01 2021-07-01   420\n",
      "✅ Saved updated censored file to: /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/Preprocces_ratio_microbiome/data_level7_censored.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "print(\"--- Processing Censored Data (Columns & Dates) ---\")\n",
    "\n",
    "# Ensure the dataframe exists\n",
    "if 'df_censored_clean' in locals():\n",
    "    \n",
    "    # 1. Drop irrelevant columns for censored data\n",
    "    cols_to_drop = ['death_date', 'death_age_month']\n",
    "    df_censored_clean = df_censored_clean.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # Ensure numeric types for calculation\n",
    "    if 'AgeMonths' in df_censored_clean.columns:\n",
    "        df_censored_clean['AgeMonths'] = pd.to_numeric(df_censored_clean['AgeMonths'], errors='coerce')\n",
    "        \n",
    "        # 2. Calculate 'diff' column\n",
    "        # Formula: (18 - AgeMonths) * 30\n",
    "        df_censored_clean['diff'] = (18 - df_censored_clean['AgeMonths']) * 30\n",
    "        \n",
    "        # 3. Calculate 'DateEnd' column\n",
    "        # Logic: SamplingDate + (18 - AgeMonths) months\n",
    "        if 'SamplingDate' in df_censored_clean.columns:\n",
    "            # Ensure SamplingDate is datetime\n",
    "            df_censored_clean['SamplingDate'] = pd.to_datetime(df_censored_clean['SamplingDate'])\n",
    "            \n",
    "            # Define a helper function to add months per row\n",
    "            def add_months_to_reach_18(row):\n",
    "                try:\n",
    "                    months_to_add = int(18 - row['AgeMonths'])\n",
    "                    return row['SamplingDate'] + DateOffset(months=months_to_add)\n",
    "                except Exception as e:\n",
    "                    return pd.NaT\n",
    "\n",
    "            # Apply calculation\n",
    "            df_censored_clean['DateEnd'] = df_censored_clean.apply(add_months_to_reach_18, axis=1)\n",
    "            \n",
    "            # Ensure final format is datetime\n",
    "            df_censored_clean['DateEnd'] = pd.to_datetime(df_censored_clean['DateEnd'])\n",
    "            \n",
    "            print(\"✅ Added 'diff' and 'DateEnd' columns.\")\n",
    "            print(df_censored_clean[['ID', 'AgeMonths', 'SamplingDate', 'DateEnd', 'diff']].head())\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ Error: 'SamplingDate' column missing.\")\n",
    "    else:\n",
    "        print(\"❌ Error: 'AgeMonths' column missing.\")\n",
    "\n",
    "    # 4. Save to file\n",
    "    df_censored_clean.to_csv(path_censored, index=False)\n",
    "    print(f\"✅ Saved updated censored file to: {path_censored}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Error: df_censored_clean is not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f88a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncensored_clean.rename(columns={'SamplingDate': 'Date'}, inplace=True)\n",
    "df_censored_clean.rename(columns={'SamplingDate': 'Date'}, inplace=True)\n",
    "df_uncensored_clean.rename(columns={'death_date': 'DateEnd'}, inplace=True)\n",
    "df_uncensored_clean.to_csv(path_uncensored, index=False)\n",
    "df_censored_clean.to_csv(path_censored, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
