{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "784e2ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Merge Process ---\n",
      "Loading Z-Train from: /home/pintokf/Projects/Microbium/Mouses/Locate_model/Whole_data/locate_Z_train_level_6.csv\n",
      "Loading Z-Test from: /home/pintokf/Projects/Microbium/Mouses/Locate_model/Whole_data/locate_Z_test_level_6.csv\n",
      "Combined Z Data Shape: (72, 11)\n",
      "ID column verified in Z data.\n",
      "Loading Metadata from: /home/pintokf/Projects/Microbium/Mouses/mouses_2_data/metadata_ok173_time_series_all.txt\n",
      "Loaded Metadata. Shape: (170, 18)\n",
      "Merging Z features with Metadata...\n",
      "--- Merge Complete ---\n",
      "Combined Z Samples: 72\n",
      "Metadata Samples: 170\n",
      "Final Merged Samples (Intersection): 72\n",
      "✅ Saved merged file to: /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/preprocces_ratio_locate/merged_Z_level6_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Paths ===\n",
    "# Input 1: The Z-score test file from LOCATE\n",
    "z_test_path = '/home/pintokf/Projects/Microbium/Mouses/Locate_model/Whole_data/locate_Z_test_level_6.csv'\n",
    "# Input 2: The Z-score train file from LOCATE\n",
    "z_train_path = '/home/pintokf/Projects/Microbium/Mouses/Locate_model/Whole_data/locate_Z_train_level_6.csv'\n",
    "\n",
    "# Input 3: The metadata file\n",
    "metadata_path = \"/home/pintokf/Projects/Microbium/Mouses/mouses_2_data/metadata_ok173_time_series_all.txt\"\n",
    "\n",
    "# Output: The final merged file containing Z-features and Metadata\n",
    "output_path = \"/home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/preprocces_ratio_locate/merged_Z_level6_metadata.csv\"\n",
    "\n",
    "print(\"--- Starting Merge Process ---\")\n",
    "\n",
    "# 1. Load and Combine Z Data (Train + Test)\n",
    "try:\n",
    "    print(f\"Loading Z-Train from: {z_train_path}\")\n",
    "    df_train = pd.read_csv(z_train_path)\n",
    "\n",
    "    print(f\"Loading Z-Test from: {z_test_path}\")\n",
    "    df_test = pd.read_csv(z_test_path)\n",
    "\n",
    "    # Concatenate them (stacking rows) since columns are identical\n",
    "    df_z_all = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    print(f\"Combined Z Data Shape: {df_z_all.shape}\")\n",
    "\n",
    "    # Verify ID column name in the Z data\n",
    "    if 'ID' in df_z_all.columns:\n",
    "        print(\"ID column verified in Z data.\")\n",
    "    elif 'SampleID' in df_z_all.columns:\n",
    "        print(\"Renaming 'SampleID' to 'ID' in Z data.\")\n",
    "        df_z_all.rename(columns={'SampleID': 'ID'}, inplace=True)\n",
    "    else:\n",
    "        # Fallback: assuming first column is ID if not named explicitly\n",
    "        print(f\"Warning: 'ID' column not found. Renaming first column '{df_z_all.columns[0]}' to 'ID'.\")\n",
    "        df_z_all.rename(columns={df_z_all.columns[0]: 'ID'}, inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Z data: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 2. Load Metadata\n",
    "try:\n",
    "    print(f\"Loading Metadata from: {metadata_path}\")\n",
    "    # Metadata is usually tab-separated\n",
    "    df_meta = pd.read_csv(metadata_path, sep='\\t')\n",
    "    print(f\"Loaded Metadata. Shape: {df_meta.shape}\")\n",
    "\n",
    "    # Standardize ID column name in Metadata\n",
    "    if '#SampleID' in df_meta.columns:\n",
    "        df_meta.rename(columns={'#SampleID': 'ID'}, inplace=True)\n",
    "    elif 'SampleID' in df_meta.columns:\n",
    "        df_meta.rename(columns={'SampleID': 'ID'}, inplace=True)\n",
    "\n",
    "    # Verify ID exists\n",
    "    if 'ID' not in df_meta.columns:\n",
    "        raise ValueError(\"Could not find ID column in metadata (expected '#SampleID' or 'SampleID')\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading metadata: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 3. Merge Tables\n",
    "# We use 'inner' merge to keep only samples that exist in BOTH (Z-features and Metadata)\n",
    "print(\"Merging Z features with Metadata...\")\n",
    "merged_df = pd.merge(df_z_all, df_meta, on='ID', how='inner')\n",
    "\n",
    "# 4. Save\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"--- Merge Complete ---\")\n",
    "print(f\"Combined Z Samples: {len(df_z_all)}\")\n",
    "print(f\"Metadata Samples: {len(df_meta)}\")\n",
    "print(f\"Final Merged Samples (Intersection): {len(merged_df)}\")\n",
    "print(f\"✅ Saved merged file to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e901d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Total: 72\n",
      "Uncensored (Dead/Events): 22\n",
      "Censored (Alive/No Event): 50\n",
      "\n",
      "✅ Files saved successfully:\n",
      "1. /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/preprocces_ratio_locate/locate_uncensored_level_6.csv\n",
      "2. /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/preprocces_ratio_locate/locate_censored_level_6.csv\n"
     ]
    }
   ],
   "source": [
    "# === Split Data based on 'Death' column ===\n",
    "\n",
    "# 1. Create the Uncensored group (Death = yes)\n",
    "df_uncensored = merged_df[merged_df['death'] == 'yes'].copy()\n",
    "\n",
    "# 2. Create the Censored group (Death = no)\n",
    "df_censored = merged_df[merged_df['death'] == 'no'].copy()\n",
    "\n",
    "# === Verification ===\n",
    "print(f\"Original Total: {len(merged_df)}\")\n",
    "print(f\"Uncensored (Dead/Events): {len(df_uncensored)}\")\n",
    "print(f\"Censored (Alive/No Event): {len(df_censored)}\")\n",
    "\n",
    "# === Save to files ===\n",
    "output_dir = \"/home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/preprocces_ratio_locate\"\n",
    "\n",
    "path_uncensored = f\"{output_dir}/locate_uncensored_level_6.csv\"\n",
    "path_censored = f\"{output_dir}/locate_censored_level_6.csv\"\n",
    "\n",
    "df_uncensored.to_csv(path_uncensored, index=False)\n",
    "df_censored.to_csv(path_censored, index=False)\n",
    "\n",
    "print(f\"\\n✅ Files saved successfully:\")\n",
    "print(f\"1. {path_uncensored}\")\n",
    "print(f\"2. {path_censored}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ccf613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Uncensored (Dead) Data ---\n",
      "✅ Extracted 'Cage' and 'MiceName' from 'mice_name'\n",
      "Preview (should be 2, 4, 6):\n",
      "           ID  AgeMonths\n",
      "4   34-0_7-20          4\n",
      "8   36-2_7-20          4\n",
      "10  20-1_5-20          4\n",
      "15  47-1_7-20          4\n",
      "17  41-1_7-20          4\n",
      "\n",
      "--- Processing Censored (Alive) Data ---\n",
      "✅ Extracted 'Cage' and 'MiceName' from 'mice_name'\n",
      "Preview (should be 2, 4, 6):\n",
      "          ID  AgeMonths\n",
      "0  29-1_5-20          4\n",
      "1  41-0_7-20          4\n",
      "2  38-2_7-20          4\n",
      "3  32-1_7-20          4\n",
      "5  42-1_5-20          2\n",
      "\n",
      "✅ Files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_mouse_data(df_input):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # === 1. Standardize Column Names ===\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Rename 'age (weeks)' to 'AgeMonths'\n",
    "    if 'age (weeks)' in df.columns:\n",
    "        df.rename(columns={'age (weeks)': 'AgeMonths'}, inplace=True)\n",
    "        \n",
    "    # Rename 'date_month' to 'SamplingDate'\n",
    "    if 'date_month' in df.columns:\n",
    "        df.rename(columns={'date_month': 'SamplingDate'}, inplace=True)\n",
    "\n",
    "    # === 2. Extract 'Cage' and 'MiceName' from 'mice_name' ===\n",
    "    if 'mice_name' in df.columns:\n",
    "        # Create 'Cage' column\n",
    "        df['Cage'] = df['mice_name'].apply(lambda x: str(x).split('-')[0] if '-' in str(x) else str(x))\n",
    "        \n",
    "        # Create 'MiceName' column\n",
    "        df['MiceName'] = df['mice_name'].astype(str).str.replace('Agf', '', case=False, regex=False)\n",
    "        df['MiceName'] = df['MiceName'].str.replace('-m', '-', regex=False)\n",
    "        \n",
    "        print(\"✅ Extracted 'Cage' and 'MiceName' from 'mice_name'\")\n",
    "\n",
    "    # === 3. Convert Weeks to Months (Using CEIL / Round UP) ===\n",
    "    # Logic: \n",
    "    # 7 weeks / 4 = 1.75 -> ceil -> 2\n",
    "    # 15 weeks / 4 = 3.75 -> ceil -> 4\n",
    "    # 23 weeks / 4 = 5.75 -> ceil -> 6\n",
    "    if 'AgeMonths' in df.columns:\n",
    "        df['AgeMonths'] = pd.to_numeric(df['AgeMonths'], errors='coerce')\n",
    "        df['AgeMonths'] = np.ceil(df['AgeMonths'] / 4).astype('Int64')\n",
    "\n",
    "    # === 4. Fix SamplingDate Format ===\n",
    "    if 'SamplingDate' in df.columns:\n",
    "        def fix_date_format(val):\n",
    "            val_str = str(val).strip()\n",
    "            if '_' in val_str:\n",
    "                parts = val_str.split('_')\n",
    "                if len(parts) == 2:\n",
    "                    return f\"01-{parts[0]}-20{parts[1]}\"\n",
    "            return val \n",
    "\n",
    "        df['SamplingDate'] = df['SamplingDate'].apply(fix_date_format)\n",
    "        df['SamplingDate'] = pd.to_datetime(df['SamplingDate'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # === 5. Drop unnecessary columns ===\n",
    "    cols_to_drop = ['barcode', 'Plate', 'LinkerPrimerSequence', 'ReversePrimer', \n",
    "                    'death', 'WellPosition', 'metabolomics', 'date', 'number', \n",
    "                    'ignore_kit', 'group', 'death_age_week',\n",
    "                    'mice_name'] \n",
    "    \n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "# === Execution ===\n",
    "\n",
    "print(\"--- Processing Uncensored (Dead) Data ---\")\n",
    "if 'df_uncensored' in locals():\n",
    "    df_uncensored_clean = process_mouse_data(df_uncensored)\n",
    "    print(\"Preview (should be 2, 4, 6):\")\n",
    "    print(df_uncensored_clean[['ID', 'AgeMonths']].head())\n",
    "\n",
    "print(\"\\n--- Processing Censored (Alive) Data ---\")\n",
    "if 'df_censored' in locals():\n",
    "    df_censored_clean = process_mouse_data(df_censored)\n",
    "    print(\"Preview (should be 2, 4, 6):\")\n",
    "    print(df_censored_clean[['ID', 'AgeMonths']].head())\n",
    "\n",
    "# === Save Files ===\n",
    "if 'path_uncensored' in locals() and 'path_censored' in locals():\n",
    "    df_uncensored_clean.to_csv(path_uncensored, index=False)\n",
    "    df_censored_clean.to_csv(path_censored, index=False)\n",
    "    print(f\"\\n✅ Files saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b743d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Uncensored Data (Dates & Diff) ---\n",
      "✅ Added 'diff' column.\n",
      "           ID  AgeMonths  death_age_month  diff\n",
      "4   34-0_7-20          4               13   270\n",
      "8   36-2_7-20          4               17   390\n",
      "10  20-1_5-20          4               12   240\n",
      "15  47-1_7-20          4               13   270\n",
      "17  41-1_7-20          4               16   360\n",
      "✅ Saved updated file to: /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/preprocces_ratio_locate/locate_uncensored_level_6.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Logic to fix DeathDate specific format ===\n",
    "# Input examples: \"05_21_b\", \"06_21\"\n",
    "# Output: 01-05-2021 (datetime object)\n",
    "def fix_death_date_format(val):\n",
    "    val_str = str(val).strip()\n",
    "    \n",
    "    if val_str == 'nan' or val_str == '':\n",
    "        return pd.NaT\n",
    "    \n",
    "    try:\n",
    "        # Split by underscore '_'\n",
    "        parts = val_str.split('_')\n",
    "        \n",
    "        # We need at least the first two parts (Month and Year)\n",
    "        if len(parts) >= 2:\n",
    "            month = parts[0]      # e.g., \"05\"\n",
    "            year_short = parts[1] # e.g., \"21\"\n",
    "            \n",
    "            # Construct the string \"01-MM-20YY\"\n",
    "            date_str = f\"01-{month}-20{year_short}\"\n",
    "            \n",
    "            # Convert to datetime\n",
    "            return pd.to_datetime(date_str, format='%d-%m-%Y')\n",
    "        else:\n",
    "            return pd.NaT\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error parsing date: {val} -> {e}\")\n",
    "        return pd.NaT\n",
    "\n",
    "# === Main Processing for Uncensored Data ===\n",
    "print(\"--- Processing Uncensored Data (Dates & Diff) ---\")\n",
    "\n",
    "# Ensure the dataframe exists\n",
    "if 'df_uncensored_clean' in locals():\n",
    "    \n",
    "    # 1. Apply DeathDate Fix\n",
    "    if 'death_date' in df_uncensored_clean.columns:\n",
    "        df_uncensored_clean['death_date'] = df_uncensored_clean['death_date'].apply(fix_death_date_format)\n",
    "\n",
    "    # 2. Calculate 'diff' Column\n",
    "    # Formula: (DeathAgeMonths - AgeMonths) * 30\n",
    "    if 'death_age_month' in df_uncensored_clean.columns and 'AgeMonths' in df_uncensored_clean.columns:\n",
    "        \n",
    "        # Ensure columns are numeric\n",
    "        df_uncensored_clean['death_age_month'] = pd.to_numeric(df_uncensored_clean['death_age_month'], errors='coerce')\n",
    "        df_uncensored_clean['AgeMonths'] = pd.to_numeric(df_uncensored_clean['AgeMonths'], errors='coerce')\n",
    "        \n",
    "        # Perform calculation\n",
    "        df_uncensored_clean['diff'] = (df_uncensored_clean['death_age_month'] - df_uncensored_clean['AgeMonths']) * 30\n",
    "        \n",
    "        print(\"✅ Added 'diff' column.\")\n",
    "        # Preview the new column\n",
    "        print(df_uncensored_clean[['ID', 'AgeMonths', 'death_age_month', 'diff']].head())\n",
    "    else:\n",
    "        print(\"❌ Error: Missing 'death_age_month' or 'AgeMonths' columns.\")\n",
    "\n",
    "    # 3. Save to file\n",
    "    df_uncensored_clean = df_uncensored_clean.drop('death_age_month', axis=1)\n",
    "    df_uncensored_clean.to_csv(path_uncensored, index=False)\n",
    "    print(f\"✅ Saved updated file to: {path_uncensored}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Error: df_uncensored_clean is not defined. Please run previous steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07003854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Censored Data (Columns & Dates) ---\n",
      "✅ Added 'diff' and 'DateEnd' columns.\n",
      "          ID  AgeMonths SamplingDate    DateEnd  diff\n",
      "0  29-1_5-20          4   2020-05-01 2021-07-01   420\n",
      "1  41-0_7-20          4   2020-07-01 2021-09-01   420\n",
      "2  38-2_7-20          4   2020-07-01 2021-09-01   420\n",
      "3  32-1_7-20          4   2020-07-01 2021-09-01   420\n",
      "5  42-1_5-20          2   2020-05-01 2021-09-01   480\n",
      "✅ Saved updated censored file to: /home/pintokf/Projects/Microbium/Mouses/Preprocess_ratio/Whole_data/preprocces_ratio_locate/locate_censored_level_6.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "print(\"--- Processing Censored Data (Columns & Dates) ---\")\n",
    "\n",
    "# Ensure the dataframe exists\n",
    "if 'df_censored_clean' in locals():\n",
    "    \n",
    "    # 1. Drop irrelevant columns for censored data\n",
    "    cols_to_drop = ['death_date', 'death_age_month']\n",
    "    df_censored_clean = df_censored_clean.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # Ensure numeric types for calculation\n",
    "    if 'AgeMonths' in df_censored_clean.columns:\n",
    "        df_censored_clean['AgeMonths'] = pd.to_numeric(df_censored_clean['AgeMonths'], errors='coerce')\n",
    "        \n",
    "        # 2. Calculate 'diff' column\n",
    "        # Formula: (18 - AgeMonths) * 30\n",
    "        df_censored_clean['diff'] = (18 - df_censored_clean['AgeMonths']) * 30\n",
    "        \n",
    "        # 3. Calculate 'DateEnd' column\n",
    "        # Logic: SamplingDate + (18 - AgeMonths) months\n",
    "        if 'SamplingDate' in df_censored_clean.columns:\n",
    "            # Ensure SamplingDate is datetime\n",
    "            df_censored_clean['SamplingDate'] = pd.to_datetime(df_censored_clean['SamplingDate'])\n",
    "            \n",
    "            # Define a helper function to add months per row\n",
    "            def add_months_to_reach_18(row):\n",
    "                try:\n",
    "                    months_to_add = int(18 - row['AgeMonths'])\n",
    "                    return row['SamplingDate'] + DateOffset(months=months_to_add)\n",
    "                except Exception as e:\n",
    "                    return pd.NaT\n",
    "\n",
    "            # Apply calculation\n",
    "            df_censored_clean['DateEnd'] = df_censored_clean.apply(add_months_to_reach_18, axis=1)\n",
    "            \n",
    "            # Ensure final format is datetime\n",
    "            df_censored_clean['DateEnd'] = pd.to_datetime(df_censored_clean['DateEnd'])\n",
    "            \n",
    "            print(\"✅ Added 'diff' and 'DateEnd' columns.\")\n",
    "            print(df_censored_clean[['ID', 'AgeMonths', 'SamplingDate', 'DateEnd', 'diff']].head())\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ Error: 'SamplingDate' column missing.\")\n",
    "    else:\n",
    "        print(\"❌ Error: 'AgeMonths' column missing.\")\n",
    "\n",
    "    # 4. Save to file\n",
    "    df_censored_clean.to_csv(path_censored, index=False)\n",
    "    print(f\"✅ Saved updated censored file to: {path_censored}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Error: df_censored_clean is not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f88a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncensored_clean.rename(columns={'SamplingDate': 'Date'}, inplace=True)\n",
    "df_censored_clean.rename(columns={'SamplingDate': 'Date'}, inplace=True)\n",
    "df_uncensored_clean.rename(columns={'death_date': 'DateEnd'}, inplace=True)\n",
    "df_uncensored_clean.to_csv(path_uncensored, index=False)\n",
    "df_censored_clean.to_csv(path_censored, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
